{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAf5ijjFjBaKtONK6XxZMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayasherby/content/blob/main/Grad_model_Tooth_segmentation%D9%89%D8%AB%D8%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKGe9_ecqvux",
        "outputId": "0eebc781-aab0-430c-ff9e-47eae7d50528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TEETH-RECOGNITION-WITH-MACHINE-LEARNING'...\n",
            "remote: Enumerating objects: 32505, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 32505 (delta 19), reused 17 (delta 9), pack-reused 32477\u001b[K\n",
            "Receiving objects: 100% (32505/32505), 6.29 GiB | 35.78 MiB/s, done.\n",
            "Resolving deltas: 100% (5099/5099), done.\n",
            "Updating files: 100% (14166/14166), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arnold0210/TEETH-RECOGNITION-WITH-MACHINE-LEARNING\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb5T0pqI8NK8",
        "outputId": "9192dc5a-cf6f-497a-ea0b-2c1934464857"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEmJCgbc1cz0",
        "outputId": "1aeb4d56-fb19-42f5-9e38-54e26b4404e1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory to your project root\n",
        "project_root = '/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment'\n",
        "os.chdir(project_root)\n",
        "\n",
        "# Verify the change\n",
        "print(\"Current Working Directory: \", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuJaF0MOIcNf",
        "outputId": "00b4f3b5-7341-48db-9e76-2929f4f2a8b7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory:  /content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BnTunXQx3sB",
        "outputId": "ab62efda-0609-4148-cb9e-404fe21438d5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreProcessing Directory Already Exists.\n",
            "FeatureExtraction Directory Already Exists.\n",
            "Sampling Directory Already Exists.\n",
            "Classification Directory Already Exists.\n",
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "SVM Directory Already Exists.\n",
            "Que desea hacer? \n",
            "1. Leer imagenes y obtener caracteristicas\n",
            "2. Leer archivo de caracteristicas y entrenar algoritmo\n",
            "2\n",
            "2\n",
            "\n",
            "Indique la cantidad de veces de ejecución:2\n",
            "  0% 0/2 [00:00<?, ? times/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/main.py\", line 254, in <module>\n",
            "    tesis.main_alldataset()\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/main.py\", line 209, in main_alldataset\n",
            "    names, features = cc.readfeatures(filefeaturespath)\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/Source/Classification.py\", line 38, in readfeatures\n",
            "    featuresFile = pd.read_csv(features_Path, sep=',', header=None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 577, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 859, in get_handle\n",
            "    handle = open(\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/../FeatureExtraction/features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main Module\n",
        "\n",
        "import errno\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import Source.Classification as Cl\n",
        "import Source.FeatureExtraction as fE\n",
        "import Source.PreProcessingData as pD\n",
        "import Source.ReadImages as rI\n",
        "\n",
        "\n",
        "def show(image):\n",
        "    cv.imshow('Imagen ', image)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyAllWindows()\n",
        "\n",
        "\n",
        "class MainClass:\n",
        "    PROJECT_PATH = os.path.join(os.getcwd(), os.path.pardir)\n",
        "    PATH_IMAGES_ORIGINAL = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET - Original\"))\n",
        "    PATH_Labels = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"))\n",
        "    PATH_LabelsXML = os.path.abspath(os.path.join(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"), \"LabelsXML\"))\n",
        "    PATH_IMAGES = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET\"))\n",
        "    readimages = None\n",
        "    preprocessing = None\n",
        "    featureExtraction = None\n",
        "    clasification = None\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'PreProcessing'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'PreProcessing'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('PreProcessing Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'FeatureExtraction'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('FeatureExtraction Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Sampling'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Sampling'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Sampling Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Classification'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Classification'))\n",
        "                print('Directory Classification Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Classification Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        self.readimages = rI.LoadData(self.PATH_IMAGES_ORIGINAL)\n",
        "        self.preProcessing = pD.PreProcessingData(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.featureExtraction = fE.FeatureExtraction(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.clasification = Cl.Classification(self.PROJECT_PATH)\n",
        "\n",
        "    def main_run(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset ORIGINAL\n",
        "        # img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset RECORTADO\n",
        "        img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "\n",
        "        # Se obtiene las dimensiones de la imagen original\n",
        "        height_ori, width_ori, depth_ori = img.shape\n",
        "        # print(\"Image original shape: \\n Height:\", height_ori, \", Width:\", width_ori)\n",
        "\n",
        "        # Se realiza un ajuste de tamaño para reducir la imagen a unas dimensiones de 600x400\n",
        "        img_resize = pp.resize_Image(img, name)\n",
        "\n",
        "        # La imagen reajustada se convierte de BGR a RGB\n",
        "        img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "        # Se convierte la imagen de RGB a HSV\n",
        "        hsv_image = pp.rgb_2_HSV(img_resize, name)\n",
        "        # Se saca la imagen en pila de tono de rojos\n",
        "        stack, name = pp.stackColors(hsv_image, name)\n",
        "        # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "        pp.hsv_hist(hsv_image, name)\n",
        "        #\n",
        "        # plt.imshow(img_resize)\n",
        "        # Blur image slightly\n",
        "        name, blurimage = pp.blurImage(img_resize, name)\n",
        "        pp.show_mask(blurimage, name)\n",
        "        pp.overlay_mask(blurimage, img_resize, name)\n",
        "\n",
        "        # Se obtiene la rueda cromatica de la imágen\n",
        "        # pp.getChromatiColor(img_resize,name,fe)\n",
        "\n",
        "        # img_rgb2ycbcr = pp.rgb_2_YCrCb(img_resize, name)\n",
        "        img_rgb2hsv = pp.rgb_2_HSV(img_resize, name)\n",
        "\n",
        "    '''easygui.msgbox(\"Image original shape: \\n Height:\" + str(height_ori) + \"px, Width:\" + str(width_ori) + \"px\" +\n",
        "                   \"\\n Image Resize shape: \\n Height:\" + str(height_res) + \"px, Width:\" + str(width_res) + \"px\",\n",
        "                   image=os.path.join(os.path.join(os.getcwd(), os.path.pardir),\n",
        "                                      'PreProcessing/ResizeImages/' + name),\n",
        "                   title=\"Image Shape - PreProcessing \")'''\n",
        "\n",
        "    def savebin(self):\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        images, names = read.read_Images(self.PATH_IMAGES_P)\n",
        "        for image_point, name_point in zip(images, names):\n",
        "            img_resize = pp.resize_Image(image_point, name_point)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_RGB2GRAY)\n",
        "            pp.bin(img_resize, name_point)\n",
        "\n",
        "    def main_alldataset(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        cc = self.clasification\n",
        "        doption = input(\n",
        "            'Que desea hacer? \\n1. Leer imagenes y obtener caracteristicas'\n",
        "            '\\n2. Leer archivo de caracteristicas y entrenar algoritmo\\n')\n",
        "        option = int(doption)\n",
        "        print(option)\n",
        "\n",
        "        if option == 1:\n",
        "            images, names = read.read_Images(self.PATH_IMAGES)\n",
        "            bar = tqdm(images, ncols=len(images), unit=' image')\n",
        "            for image_point, name_point in zip(bar, names):\n",
        "                bar.set_description(\"Procesando imagen %s\" % name_point)\n",
        "                # Se reajusta la imagen a un tamaño de 600x400px\n",
        "                img_resize = pp.resize_Image(image_point, name_point)\n",
        "\n",
        "                # La imagen reajustada se convierte de BGR a RGB\n",
        "                img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "\n",
        "                # Se convierte la imagen de RGB a HSV\n",
        "                hsv_image = pp.rgb_2_HSV(img_resize, name_point)\n",
        "\n",
        "                # Se saca la imagen en pila de tono de rojos\n",
        "                stack, name_point = pp.stackColors(hsv_image, name_point)\n",
        "\n",
        "                # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "                pp.hsv_hist(hsv_image, name_point)\n",
        "\n",
        "                # Blur image slightly\n",
        "                name_point, blurimage = pp.blurImage(img_resize, name_point)\n",
        "                '''file_ = open(os.path.join(self.PROJECT_PATH, 'Pruebas') + name_point + '.txt', \"w\")\n",
        "                for i in blurimage:\n",
        "                    file_.write(str(i))\n",
        "                file_.close()'''\n",
        "\n",
        "                # A partir del rango de color, se saca una máscara donde se ubican los dientes y se procede a buscar el contorno más grande dentro del área objetivo.\n",
        "                mask = pp.findBiggestContour(blurimage, name_point)\n",
        "                # Se separan los canales de la\n",
        "                # imágen en RGB\n",
        "                channelR, channelG, channelB = cv.split(img_resize)\n",
        "                # Se obtienen los momentos de color de cada espacio de color\n",
        "                red = fe.getFeaturesVector(channelR, mask)\n",
        "                green = fe.getFeaturesVector(channelG, mask)\n",
        "                blue = fe.getFeaturesVector(channelB, mask)\n",
        "                # colores = [\"RED\", \"GREEN\", \"BLUE\"]\n",
        "\n",
        "                # Se obtiene una mascara de la imágen compuesta por los valores de los canales donde se encontraron los datos de interes.\n",
        "                imagen = [red, green, blue]\n",
        "                # Se coloca la ruta del archivo de caracteristicas\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                # Se escribe en un archivo las caracteristicas del dataset\n",
        "                fe.getFeatures(imagen, filefeaturespath, name_point)\n",
        "                fileLabels = open(os.path.join(self.PATH_Labels, 'Labels.csv'))\n",
        "                pp.show_mask(blurimage, name_point)\n",
        "                pp.overlay_mask(blurimage, img_resize, name_point)\n",
        "        elif option == 2:\n",
        "            times_execution = -1\n",
        "            while times_execution <= 0:\n",
        "                times_execution = int(input('\\nIndique la cantidad de veces de ejecución:'))\n",
        "            bar = tqdm(range(times_execution), unit=' times')\n",
        "            for i in bar:\n",
        "                # Se lee el archivo de caracteristicas donde se encuentran los momentos de color\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                #filefeaturespath= \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/FeatureExtraction/features.csv\"\n",
        "                names, features = cc.readfeatures(filefeaturespath)\n",
        "                # Change the path to the labels file\n",
        "                tesis.PATH_Labels = \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Labels\"\n",
        "                labels = cc.readLabels(self.PATH_Labels)\n",
        "                features_images = features.values\n",
        "                vals_to_replace = {'a1': '0', 'a2': '1', 'a3': '2', 'a35': '3'}\n",
        "                tags = ['0', '1', '2', '3']\n",
        "                target_names = ['a1', 'a2', 'a3', 'a35']\n",
        "                folds = int(input('\\nCantidad de folios a seperarar el conjunto de datos:'))\n",
        "                test_size = int(input('\\n Porcentaje de division del conjunto de datos trianing/test:'))\n",
        "                X, Y = cc.CrossValidation(features, labels, test_size / 100)\n",
        "                SVM, DT, KNN = cc.classification(self.PATH_IMAGES, X, Y, folds, tags, target_names, vals_to_replace)\n",
        "                print('--------- Training ---------')\n",
        "                for S, D, K in zip(SVM, DT, KNN):\n",
        "\n",
        "                    matrix_confusion_SVM, report_clasification_SVM, report_scores_SVM = S.split()\n",
        "                    matrix_confusion_DT, report_clasification_DT, report_scores_DT = D.split()\n",
        "                    matrix_confusion_KNN, report_clasification_KNN, report_scores_KNN = K.split()\n",
        "                    print('\\n')\n",
        "                    print('-------SVM------')\n",
        "                    for report in report_clasification_SVM:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN SVM--------')\n",
        "                    print(np.mean(report_scores_SVM))\n",
        "                    print('-------DT------')\n",
        "                    for report in report_clasification_DT:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN DT--------')\n",
        "                    print(np.mean(report_scores_DT))\n",
        "\n",
        "                    print('-------KNN------')\n",
        "                    for report in report_clasification_KNN:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN KNN--------')\n",
        "                    print(np.mean(report_scores_KNN))\n",
        "                    print('--------- TEST ---------')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tesis: MainClass = MainClass()\n",
        "    # tesis.main_run()\n",
        "    tesis.main_alldataset()\n",
        "    # tesis.savebin()\n",
        "    print('Se ha finalizado la ejecución del experimento')\n",
        "    sys.exit(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ryFFV2Ts8USA",
        "outputId": "1b644b86-78f7-41f5-b8c9-7186cf8196b3"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PreProcessing Directory Already Exists.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PreProcessing Directory Already Exists.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FeatureExtraction Directory Already Exists.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FeatureExtraction Directory Already Exists.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sampling Directory Already Exists.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sampling Directory Already Exists.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Classification Directory Already Exists.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Classification Directory Already Exists.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "SVM Directory Already Exists.\n",
            "Que desea hacer? \n",
            "1. Leer imagenes y obtener caracteristicas\n",
            "2. Leer archivo de caracteristicas y entrenar algoritmo\n",
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Indique la cantidad de veces de ejecución:10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ? times/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cantidad de folios a seperarar el conjunto de datos:10\n",
            "\n",
            " Porcentaje de division del conjunto de datos trianing/test:80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:02<?, ? times/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'builtin_function_or_method' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-f7f4537eb789>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mtesis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMainClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMainClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# tesis.main_run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mtesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_alldataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;31m# tesis.savebin()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Se ha finalizado la ejecución del experimento'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-159-f7f4537eb789>\u001b[0m in \u001b[0;36mmain_alldataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Porcentaje de division del conjunto de datos trianing/test:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mSVM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPATH_IMAGES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals_to_replace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------- Training ---------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/Source/Classification.py\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(self, path_dataset, features, labels, n_splits, tags, target_names, vals_to_replace)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                    \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monlyfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                    \u001b[0mimage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classification class**\n"
      ],
      "metadata": {
        "id": "9DAskc84OgB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Module\n",
        "import errno\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "class Classification:\n",
        "    def __init__(self, PATH_PROJECT):\n",
        "        self.path_project = os.path.join(PATH_PROJECT, 'Classification')\n",
        "        try:\n",
        "            if not os.path.exists(self.path_project + 'SVM'):\n",
        "                self.path_getColor = os.path.join(self.path_project, 'SVM')\n",
        "                os.mkdir(self.path_getColor)\n",
        "                print('SVM Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('SVM Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    def readfeatures(self, features_Path):\n",
        "\n",
        "        featuresFile = pd.read_csv(features_Path, sep=',', header=None)\n",
        "        names = featuresFile.iloc[:, 0]\n",
        "        features = featuresFile.iloc[:, 1:]\n",
        "        shapefile = featuresFile.shape\n",
        "        col = []\n",
        "        for x in range(0, shapefile[1]):\n",
        "            if x == 0:\n",
        "                col.append(\"NAME\")\n",
        "            else:\n",
        "                col.append(\"VALOR-\" + str(x))\n",
        "        featuresFile.columns = col\n",
        "        # print(featuresFile)\n",
        "        return names, features\n",
        "\n",
        "    def readLabels(self, labels_path):\n",
        "        labels_path = os.path.join(labels_path, 'Labels.csv')\n",
        "        labels = pd.read_csv(labels_path, sep=',', header=[0])\n",
        "        return labels\n",
        "\n",
        "    def classificatorSVM(self, features, labels):\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        clf = svm.SVC(gamma='scale')\n",
        "        clf.fit(X, tags)\n",
        "        return clf\n",
        "\n",
        "    def DecisionTree(self, features, labels):\n",
        "        dt = DecisionTreeClassifier(random_state=30, max_depth=300)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        dt.fit(X, tags)\n",
        "\n",
        "        return dt\n",
        "\n",
        "    def KNN(self, features, labels):\n",
        "        knn = KNeighborsClassifier(n_neighbors=200, algorithm='auto', weights='distance', n_jobs=-1)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        knn.fit(X, tags)\n",
        "        return knn\n",
        "\n",
        "    def classification(self, path_dataset, features, labels, n_splits, tags, target_names, vals_to_replace):\n",
        "        pd.options.mode.chained_assignment = None\n",
        "        test_features = []\n",
        "        onlyfiles = [f for f in listdir(path_dataset) if\n",
        "                     isfile(join(path_dataset, f))]\n",
        "        print(onlyfiles)\n",
        "        k_folds = KFold(n_splits=n_splits)\n",
        "        SVM = []\n",
        "        KNN = []\n",
        "        DT = []\n",
        "        for stage, feature in zip(labels, features):\n",
        "            stage['Color'] = stage['Color'].map(vals_to_replace)\n",
        "            labels_color = stage['Color'].to_numpy().tolist()\n",
        "            images_name = stage['Nombre de la imagen'].to_numpy().tolist()\n",
        "            svm_training_Score = []\n",
        "            confusion_matrix_svm = []\n",
        "            confusion_matrix_dt = []\n",
        "            confusion_matrix_knn = []\n",
        "            classification_report_svm = []\n",
        "            classification_report_dt = []\n",
        "            classification_report_knn = []\n",
        "            score_accuracy_SVM = []\n",
        "            score_accuracy_DT = []\n",
        "            score_accuracy_KNN = []\n",
        "            index_images_name = []\n",
        "            k_folds.get_n_splits(index_images_name)\n",
        "            #print(list(images_name.index(images_name))\n",
        "            k_folds = KFold(n_splits=n_splits)\n",
        "            for train_index, test_index in k_folds.split(images_name):\n",
        "                train_label = []\n",
        "                test_label = []\n",
        "                train_features = []\n",
        "                test_features = []\n",
        "                for i in train_index:\n",
        "                    #train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                    #train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])()])\n",
        "                    train_features.append(feature.to_numpy(images_name.str(onlyfiles[i].split('.')[0])))\n",
        "                    train_label.append(labels_color[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                SVM_Classifier = self.classificatorSVM(train_features, train_label)\n",
        "                DT_Classifier = self.DecisionTree(train_features, train_label)\n",
        "                KNN_Classifier = self.KNN(train_features, train_label)\n",
        "\n",
        "                for i in test_index:\n",
        "                    test_features.append(feature.to_numpy()[images_name.index(str(onlyfiles[i].split('.')[0]))])\n",
        "                    test_label.append(labels_color[images_name.index(str(onlyfiles[i].split('.')[0]))])\n",
        "                predict_label_SVM = SVM_Classifier.predict(test_features)\n",
        "                predict_label_DT = DT_Classifier.predict(test_features)\n",
        "                predict_label_KNN = KNN_Classifier.predict(test_features)\n",
        "\n",
        "                confusionMatrixSVM = confusion_matrix(test_label, predict_label_SVM)\n",
        "                confusionMatrixDT = confusion_matrix(test_label, predict_label_DT)\n",
        "                confusionMatrixKNN = confusion_matrix(test_label, predict_label_KNN)\n",
        "\n",
        "                classification_report_svm.append(\n",
        "                    classification_report(test_label, predict_label_SVM, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_dt.append(\n",
        "                    classification_report(test_label, predict_label_DT, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_knn.append(\n",
        "                    classification_report(test_label, predict_label_KNN, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "\n",
        "                confusion_matrix_svm.append(confusionMatrixSVM)\n",
        "                confusion_matrix_dt.append(confusionMatrixDT)\n",
        "                confusion_matrix_knn.append(confusionMatrixKNN)\n",
        "\n",
        "                score_accuracy_SVM.append(accuracy_score(test_label, predict_label_SVM))\n",
        "                score_accuracy_DT.append(accuracy_score(test_label, predict_label_DT))\n",
        "                score_accuracy_KNN.append(accuracy_score(test_label, predict_label_KNN))\n",
        "            SVM_RESULTS = [confusion_matrix_svm, classification_report_svm, score_accuracy_SVM]\n",
        "            DT_RESULTS = [confusion_matrix_dt, classification_report_dt, score_accuracy_DT]\n",
        "            KNN_RESULTS = [confusion_matrix_knn, classification_report_knn, score_accuracy_KNN]\n",
        "            SVM.append(SVM_RESULTS)\n",
        "            KNN.append(KNN_RESULTS)\n",
        "            DT.append(DT_RESULTS)\n",
        "        return SVM, KNN, DT\n",
        "\n",
        "    def CrossValidation(self, image, labels, test_size):\n",
        "        X = []\n",
        "        Y = []\n",
        "        X_train, X_test, y_train, y_test = train_test_split(image, labels, test_size=test_size)\n",
        "        X.append(X_train)\n",
        "        X.append(X_test)\n",
        "        Y.append(y_train)\n",
        "        Y.append(y_test)\n",
        "        print()\n",
        "        return X, Y\n",
        "\n",
        "    def ROC_CURVE(self, label_test, label_score):\n",
        "        n_classes = label_test.shape\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(label_test[:, i], label_score[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        # Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_test.ravel(), label_score.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "        # Plot of a ROC curve for a specific class\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver operating characteristic example')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                       ''.format(roc_auc[\"micro\"]))\n",
        "        for i in range(n_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i, roc_auc[i]))\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "bTcQSKOyOcud"
      },
      "execution_count": 161,
      "outputs": []
    }
  ]
}