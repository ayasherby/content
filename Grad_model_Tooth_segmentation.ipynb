{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9DAskc84OgB8"
      ],
      "authorship_tag": "ABX9TyPAna/2+29bMH4PAF8CGHF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayasherby/content/blob/main/Grad_model_Tooth_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKGe9_ecqvux",
        "outputId": "0eebc781-aab0-430c-ff9e-47eae7d50528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TEETH-RECOGNITION-WITH-MACHINE-LEARNING'...\n",
            "remote: Enumerating objects: 32505, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 32505 (delta 19), reused 17 (delta 9), pack-reused 32477\u001b[K\n",
            "Receiving objects: 100% (32505/32505), 6.29 GiB | 35.78 MiB/s, done.\n",
            "Resolving deltas: 100% (5099/5099), done.\n",
            "Updating files: 100% (14166/14166), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arnold0210/TEETH-RECOGNITION-WITH-MACHINE-LEARNING\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb5T0pqI8NK8",
        "outputId": "9192dc5a-cf6f-497a-ea0b-2c1934464857"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEmJCgbc1cz0",
        "outputId": "1aeb4d56-fb19-42f5-9e38-54e26b4404e1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory to your project root\n",
        "project_root = '/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment'\n",
        "os.chdir(project_root)\n",
        "\n",
        "# Verify the change\n",
        "print(\"Current Working Directory: \", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "fuJaF0MOIcNf",
        "outputId": "c966b48f-60a9-4278-816e-4aabee736cc7"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Current Working Directory:  \u001b[35m/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/\u001b[0m\u001b[95mExperiment\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Current Working Directory:  <span style=\"color: #800080; text-decoration-color: #800080\">/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Experiment</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3kp0HWZdRhN",
        "outputId": "9f0ed5ce-531b-4b4a-f974-ac4cc802b332"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreProcessing Directory Already Exists.\n",
            "FeatureExtraction Directory Already Exists.\n",
            "Sampling Directory Already Exists.\n",
            "Classification Directory Already Exists.\n",
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "Segmentation Directory Already Exists.\n",
            "BarPlot Directory Already Exists.\n",
            "BarPlotCh1 Directory Already Exists.\n",
            "BarPlotCh2 Directory Already Exists.\n",
            "BarPlotCh3 Directory Already Exists.\n",
            "Mask Directory Already Exists.\n",
            "MaskOverlay Directory Already Exists.\n",
            "Inverse Directory Already Exists.\n",
            "ResizeImages Directory Already Exists.\n",
            "RGB2YCbCr Directory Already Exists.\n",
            "HSV Directory Already Exists.\n",
            "LAB Directory Already Exists.\n",
            "Lab_ Directory Already Exists.\n",
            "SVM Directory Already Exists.\n",
            "Que desea hacer? \n",
            "1. Leer imagenes y obtener caracteristicas\n",
            "2. Leer archivo de caracteristicas y entrenar algoritmo\n",
            "2\n",
            "2\n",
            "\n",
            "Indique la cantidad de veces de ejecución:5\n",
            "  0% 0/5 [00:00<?, ? times/s]Length of features 973 \n",
            "\n",
            "Length of Labels 973 \n",
            "\n",
            "\n",
            "Cantidad de folios a seperarar el conjunto de datos:5\n",
            "\n",
            " Porcentaje de division del conjunto de datos trianing/test:80\n",
            "\n",
            "\n",
            "Length of features-----> 2 \n",
            "\n",
            "\n",
            "Length of Labels-----> 2 \n",
            "\n",
            "\n",
            "-------------------------*************************************\n",
            "\n",
            "\n",
            " Length of train  index 155 <class 'numpy.ndarray'> \n",
            "\n",
            "\n",
            "Length of train_features 0 <class 'list'> \n",
            "\n",
            "\n",
            "-------------------------*************************************\n",
            "\n",
            "\n",
            "-------------------------*************************************\n",
            "\n",
            "\n",
            " Length of  test_features 0 <class 'list'> \n",
            "\n",
            "Length of test index 39 <class 'numpy.ndarray'> \n",
            "\n",
            "\n",
            "-------------------------*************************************\n",
            "\n",
            "\n",
            "\n",
            " <-----Length of i ---> 39 <class 'numpy.int64'> \n",
            "\n",
            "  0% 0/5 [00:03<?, ? times/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/main.py\", line 251, in <module>\n",
            "    tesis.main_alldataset()\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/main.py\", line 215, in main_alldataset\n",
            "    SVM, DT, KNN = cc.classification(self.PATH_IMAGES, X, Y, folds, tags, target_names, vals_to_replace)\n",
            "  File \"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/Source/Classification.py\", line 135, in classification\n",
            "    train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])])\n",
            "ValueError: '101_0542' is not in list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Saved codes to run`**"
      ],
      "metadata": {
        "id": "U4Aw3PBQTFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Main module\n",
        "\n",
        "import errno\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import Source.Classification as Cl\n",
        "import Source.FeatureExtraction as fE\n",
        "import Source.PreProcessingData as pD\n",
        "import Source.ReadImages as rI\n",
        "\n",
        "\n",
        "def show(image):\n",
        "    cv.imshow('Imagen ', image)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyAllWindows()\n",
        "\n",
        "\n",
        "class MainClass:\n",
        "    PROJECT_PATH = os.path.join(os.getcwd(), os.path.pardir)\n",
        "    PATH_IMAGES_ORIGINAL = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET - Original\"))\n",
        "    PATH_Labels = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"))\n",
        "    PATH_LabelsXML = os.path.abspath(os.path.join(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"), \"LabelsXML\"))\n",
        "    PATH_IMAGES = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET\"))\n",
        "    readimages = None\n",
        "    preprocessing = None\n",
        "    featureExtraction = None\n",
        "    clasification = None\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'PreProcessing'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'PreProcessing'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('PreProcessing Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'FeatureExtraction'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('FeatureExtraction Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Sampling'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Sampling'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Sampling Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Classification'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Classification'))\n",
        "                print('Directory Classification Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Classification Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        self.readimages = rI.LoadData(self.PATH_IMAGES_ORIGINAL)\n",
        "        self.preProcessing = pD.PreProcessingData(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.featureExtraction = fE.FeatureExtraction(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.clasification = Cl.Classification(self.PROJECT_PATH)\n",
        "\n",
        "    def main_run(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset ORIGINAL\n",
        "        # img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset RECORTADO\n",
        "        img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "\n",
        "        # Se obtiene las dimensiones de la imagen original\n",
        "        height_ori, width_ori, depth_ori = img.shape\n",
        "        # print(\"Image original shape: \\n Height:\", height_ori, \", Width:\", width_ori)\n",
        "\n",
        "        # Se realiza un ajuste de tamaño para reducir la imagen a unas dimensiones de 600x400\n",
        "        img_resize = pp.resize_Image(img, name)\n",
        "\n",
        "        # La imagen reajustada se convierte de BGR a RGB\n",
        "        img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "        # Se convierte la imagen de RGB a HSV\n",
        "        hsv_image = pp.rgb_2_HSV(img_resize, name)\n",
        "        # Se saca la imagen en pila de tono de rojos\n",
        "        stack, name = pp.stackColors(hsv_image, name)\n",
        "        # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "        pp.hsv_hist(hsv_image, name)\n",
        "        #\n",
        "        # plt.imshow(img_resize)\n",
        "        # Blur image slightly\n",
        "        name, blurimage = pp.blurImage(img_resize, name)\n",
        "        pp.show_mask(blurimage, name)\n",
        "        pp.overlay_mask(blurimage, img_resize, name)\n",
        "\n",
        "        # Se obtiene la rueda cromatica de la imágen\n",
        "        # pp.getChromatiColor(img_resize,name,fe)\n",
        "\n",
        "        # img_rgb2ycbcr = pp.rgb_2_YCrCb(img_resize, name)\n",
        "        img_rgb2hsv = pp.rgb_2_HSV(img_resize, name)\n",
        "\n",
        "    '''easygui.msgbox(\"Image original shape: \\n Height:\" + str(height_ori) + \"px, Width:\" + str(width_ori) + \"px\" +\n",
        "                   \"\\n Image Resize shape: \\n Height:\" + str(height_res) + \"px, Width:\" + str(width_res) + \"px\",\n",
        "                   image=os.path.join(os.path.join(os.getcwd(), os.path.pardir),\n",
        "                                      'PreProcessing/ResizeImages/' + name),\n",
        "                   title=\"Image Shape - PreProcessing \")'''\n",
        "\n",
        "    def savebin(self):\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        images, names = read.read_Images(self.PATH_IMAGES_P)\n",
        "        for image_point, name_point in zip(images, names):\n",
        "            img_resize = pp.resize_Image(image_point, name_point)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_RGB2GRAY)\n",
        "            pp.bin(img_resize, name_point)\n",
        "\n",
        "    def main_alldataset(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        cc = self.clasification\n",
        "        doption = input(\n",
        "            'Que desea hacer? \\n1. Leer imagenes y obtener caracteristicas'\n",
        "            '\\n2. Leer archivo de caracteristicas y entrenar algoritmo\\n')\n",
        "        option = int(doption)\n",
        "        print(option)\n",
        "\n",
        "        if option == 1:\n",
        "            images, names = read.read_Images(self.PATH_IMAGES)\n",
        "            bar = tqdm(images, ncols=len(images), unit=' image')\n",
        "            for image_point, name_point in zip(bar, names):\n",
        "                bar.set_description(\"Procesando imagen %s\" % name_point)\n",
        "                # Se reajusta la imagen a un tamaño de 600x400px\n",
        "                img_resize = pp.resize_Image(image_point, name_point)\n",
        "\n",
        "                # La imagen reajustada se convierte de BGR a RGB\n",
        "                img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "\n",
        "                # Se convierte la imagen de RGB a HSV\n",
        "                hsv_image = pp.rgb_2_HSV(img_resize, name_point)\n",
        "\n",
        "                # Se saca la imagen en pila de tono de rojos\n",
        "                stack, name_point = pp.stackColors(hsv_image, name_point)\n",
        "\n",
        "                # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "                pp.hsv_hist(hsv_image, name_point)\n",
        "\n",
        "                # Blur image slightly\n",
        "                name_point, blurimage = pp.blurImage(img_resize, name_point)\n",
        "                '''file_ = open(os.path.join(self.PROJECT_PATH, 'Pruebas') + name_point + '.txt', \"w\")\n",
        "                for i in blurimage:\n",
        "                    file_.write(str(i))\n",
        "                file_.close()'''\n",
        "\n",
        "                # A partir del rango de color, se saca una máscara donde se ubican los dientes y se procede a buscar el contorno más grande dentro del área objetivo.\n",
        "                mask = pp.findBiggestContour(blurimage, name_point)\n",
        "                # Se separan los canales de la\n",
        "                # imágen en RGB\n",
        "                channelR, channelG, channelB = cv.split(img_resize)\n",
        "                # Se obtienen los momentos de color de cada espacio de color\n",
        "                red = fe.getFeaturesVector(channelR, mask)\n",
        "                green = fe.getFeaturesVector(channelG, mask)\n",
        "                blue = fe.getFeaturesVector(channelB, mask)\n",
        "                # colores = [\"RED\", \"GREEN\", \"BLUE\"]\n",
        "\n",
        "                # Se obtiene una mascara de la imágen compuesta por los valores de los canales donde se encontraron los datos de interes.\n",
        "                imagen = [red, green, blue]\n",
        "                # Se coloca la ruta del archivo de caracteristicas\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                # Se escribe en un archivo las caracteristicas del dataset\n",
        "                fe.getFeatures(imagen, filefeaturespath, name_point)\n",
        "                fileLabels = open(os.path.join(self.PATH_Labels, 'Labels.csv'))\n",
        "                pp.show_mask(blurimage, name_point)\n",
        "                pp.overlay_mask(blurimage, img_resize, name_point)\n",
        "        elif option == 2:\n",
        "            times_execution = -1\n",
        "            while times_execution <= 0:\n",
        "                times_execution = int(input('\\nIndique la cantidad de veces de ejecución:'))\n",
        "            bar = tqdm(range(times_execution), unit=' times')\n",
        "            for i in bar:\n",
        "                # Se lee el archivo de caracteristicas donde se encuentran los momentos de color\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                names, features = cc.readfeatures(filefeaturespath)\n",
        "                labels = cc.readLabels(self.PATH_Labels)\n",
        "                features_images = features.values\n",
        "                vals_to_replace = {'a1': '0', 'a2': '1', 'a3': '2', 'a35': '3'}\n",
        "                tags = ['0', '1', '2', '3']\n",
        "                target_names = ['a1', 'a2', 'a3', 'a35']\n",
        "                folds = int(input('\\nCantidad de folios a seperarar el conjunto de datos:'))\n",
        "                test_size = int(input('\\n Porcentaje de division del conjunto de datos trianing/test:'))\n",
        "                X, Y = cc.CrossValidation(features, labels, test_size / 100)\n",
        "                SVM, DT, KNN = cc.classification(self.PATH_IMAGES, X, Y, folds, tags, target_names, vals_to_replace)\n",
        "                print('--------- Training ---------')\n",
        "                for S, D, K in zip(SVM, DT, KNN):\n",
        "\n",
        "                    matrix_confusion_SVM, report_clasification_SVM, report_scores_SVM = S.split()\n",
        "                    matrix_confusion_DT, report_clasification_DT, report_scores_DT = D.split()\n",
        "                    matrix_confusion_KNN, report_clasification_KNN, report_scores_KNN = K.split()\n",
        "                    print('\\n')\n",
        "                    print('-------SVM------')\n",
        "                    for report in report_clasification_SVM:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN SVM--------')\n",
        "                    print(np.mean(report_scores_SVM))\n",
        "                    print('-------DT------')\n",
        "                    for report in report_clasification_DT:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN DT--------')\n",
        "                    print(np.mean(report_scores_DT))\n",
        "\n",
        "                    print('-------KNN------')\n",
        "                    for report in report_clasification_KNN:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN KNN--------')\n",
        "                    print(np.mean(report_scores_KNN))\n",
        "                    print('--------- TEST ---------')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tesis: MainClass = MainClass()\n",
        "    # tesis.main_run()\n",
        "    tesis.main_alldataset()\n",
        "    # tesis.savebin()\n",
        "    print('Se ha finalizado la ejecución del experimento')\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "Zl26503ZS2Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction module\n",
        "import errno\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import Source.PreProcessingData as pD\n",
        "import Source.ReadImages as rI\n",
        "\n",
        "\n",
        "class FeatureExtraction:\n",
        "    readImages = None\n",
        "    preProcessing = None\n",
        "\n",
        "    def __init__(self, PATH_PROJECT, PATH_IMAGES):\n",
        "        self.path_project = os.path.join(PATH_PROJECT, 'FeatureExtraction')\n",
        "        self.path_dataset = PATH_IMAGES\n",
        "        try:\n",
        "            if not os.path.exists(self.path_project + 'GetColors'):\n",
        "                self.path_getColor = os.path.join(self.path_project, 'GetColors')\n",
        "                os.mkdir(self.path_getColor)\n",
        "\n",
        "                print('ResizeImages Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('ResizeImages Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        self.preProcessing = pD.PreProcessingData(PATH_PROJECT, PATH_IMAGES)\n",
        "        self.readImages = rI.LoadData(PATH_PROJECT)\n",
        "\n",
        "    def RGB2HEX(self, color):\n",
        "        return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "    def get_colors(self, src, number_of_colors, show_chart, name):\n",
        "        modified_image = src.reshape(src.shape[0] * src.shape[1], 3)\n",
        "        clf = KMeans(n_clusters=number_of_colors)\n",
        "        labels = clf.fit_predict(modified_image)\n",
        "        counts = Counter(labels)\n",
        "        center_colors = clf.cluster_centers_\n",
        "        # We get ordered colors by iterating through the keys\n",
        "        ordered_colors = [center_colors[i] for i in counts.keys()]\n",
        "        hex_colors = [self.RGB2HEX(ordered_colors[i]) for i in counts.keys()]\n",
        "        rgb_colors = [ordered_colors[i] for i in counts.keys()]\n",
        "\n",
        "        if show_chart:\n",
        "            path_image_name = os.path.join(self.path_getColor, name)\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.pie(counts.values(), labels=hex_colors, colors=hex_colors)\n",
        "            plt.savefig(path_image_name)\n",
        "            plt.show()\n",
        "\n",
        "        return rgb_colors, hex_colors\n",
        "\n",
        "    def getFeaturesVector(self, image, mask):\n",
        "        features = []\n",
        "        imagecpy = image.copy()\n",
        "        for i in range(len(mask)):\n",
        "            for j in range(len(mask[i])):\n",
        "                if (j == 255):\n",
        "                    # print(image[i][j])\n",
        "                    features.append(imagecpy[i][j])\n",
        "        return features\n",
        "\n",
        "    def meanVector(self, vector_caracteristicas):\n",
        "        return np.mean(vector_caracteristicas)\n",
        "\n",
        "    def varVector(self, vector_caracteristicas):\n",
        "        return stats.moment(vector_caracteristicas)\n",
        "\n",
        "    def skewVector(self, vector_caracteristicas):\n",
        "        return stats.skew(vector_caracteristicas)\n",
        "\n",
        "    def getFeatures(self, imagen, filefeaturespath, name_point):\n",
        "        if not (os.path.exists(filefeaturespath) or os.path.isfile(filefeaturespath)):\n",
        "            filefeatures = open(filefeaturespath, \"w\")\n",
        "        else:\n",
        "            filefeatures = open(filefeaturespath, \"a\")\n",
        "        features = []\n",
        "        for j in range(0, len(imagen)):\n",
        "            features.append(self.meanVector(imagen[j]))\n",
        "            features.append(self.varVector(imagen[j]))\n",
        "            features.append(self.skewVector(imagen[j]))\n",
        "        filefeatures.write(name_point)\n",
        "        for item in range(len(features)):\n",
        "            filefeatures.write(\",%.6f\" % features[item])\n",
        "        filefeatures.write(\"\\n\")#\",\" + label +'''\n",
        "        filefeatures.close()\n"
      ],
      "metadata": {
        "id": "AAGKmrbUd3Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification module\n",
        "import errno\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "class Classification:\n",
        "    def __init__(self, PATH_PROJECT):\n",
        "        self.path_project = os.path.join(PATH_PROJECT, 'Classification')\n",
        "        try:\n",
        "            if not os.path.exists(self.path_project + 'SVM'):\n",
        "                self.path_getColor = os.path.join(self.path_project, 'SVM')\n",
        "                os.mkdir(self.path_getColor)\n",
        "                print('SVM Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('SVM Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    def readfeatures(self, features_Path):\n",
        "        # featuresFile = open(features_Path, \"r\")\n",
        "        # featuresData = csv.reader(featuresFile)\n",
        "        featuresFile = pd.read_csv(features_Path, sep=',', header=None)\n",
        "        names = featuresFile.iloc[:, 0]\n",
        "        features = featuresFile.iloc[:, 1:]\n",
        "        shapefile = featuresFile.shape\n",
        "        print(\"Length of features\",len(features),\"\\n\")\n",
        "        col = []\n",
        "        for x in range(0, shapefile[1]):\n",
        "            if x == 0:\n",
        "                col.append(\"NAME\")\n",
        "            else:\n",
        "                col.append(\"VALOR-\" + str(x))\n",
        "        featuresFile.columns = col\n",
        "        # print(featuresFile)\n",
        "        return names, features\n",
        "\n",
        "    def readLabels(self, labels_path):\n",
        "        #labels_path = os.path.join(labels_path, 'Labels.csv')\n",
        "        labels_path=\"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/Labels/Labels.csv\"\n",
        "        labels = pd.read_csv(labels_path, sep=',', header=[0])\n",
        "        print(\"Length of Labels\",len(labels),\"\\n\")\n",
        "        return labels\n",
        "\n",
        "    def classificatorSVM(self, features, labels):\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        clf = svm.SVC(gamma='scale')\n",
        "        clf.fit(X, tags)\n",
        "        return clf\n",
        "\n",
        "    def DecisionTree(self, features, labels):\n",
        "        dt = DecisionTreeClassifier(random_state=30, max_depth=300)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        dt.fit(X, tags)\n",
        "\n",
        "        return dt\n",
        "\n",
        "    def KNN(self, features, labels):\n",
        "        knn = KNeighborsClassifier(n_neighbors=200, algorithm='auto', weights='distance', n_jobs=-1)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        knn.fit(X, tags)\n",
        "        return knn\n",
        "\n",
        "    def classification(self, path_dataset, features, labels, n_splits, tags, target_names, vals_to_replace):\n",
        "        pd.options.mode.chained_assignment = None\n",
        "        path_dataset=\"/content/TEETH-RECOGNITION-WITH-MACHINE-LEARNING/Experiment/DATASET\"\n",
        "        onlyfiles = [f for f in listdir(path_dataset) if\n",
        "                     isfile(join(path_dataset, f))]\n",
        "        #print(onlyfiles)\n",
        "        k_folds = KFold(n_splits=n_splits)\n",
        "        SVM = []\n",
        "        KNN = []\n",
        "        DT = []\n",
        "        for stage, feature in zip(labels, features):\n",
        "            stage['Color'] = stage['Color'].map(vals_to_replace)\n",
        "            labels_color = stage['Color'].to_numpy().tolist()\n",
        "            images_name = stage['Nombre de la imagen'].to_numpy().tolist()\n",
        "            svm_training_Score = []\n",
        "            confusion_matrix_svm = []\n",
        "            confusion_matrix_dt = []\n",
        "            confusion_matrix_knn = []\n",
        "            classification_report_svm = []\n",
        "            classification_report_dt = []\n",
        "            classification_report_knn = []\n",
        "            score_accuracy_SVM = []\n",
        "            score_accuracy_DT = []\n",
        "            score_accuracy_KNN = []\n",
        "            index_images_name = []\n",
        "            k_folds.get_n_splits(index_images_name)\n",
        "            for train_index, test_index in k_folds.split(images_name):\n",
        "                print(\"\\nLength of features----->\",len(features),\"\\n\")\n",
        "                print(\"\\nLength of Labels----->\",len(labels),\"\\n\")\n",
        "                train_label = []\n",
        "                test_label = []\n",
        "                train_features = []\n",
        "                print(\"\\n-------------------------*************************************\\n\")\n",
        "                print(\"\\n Length of train  index\",len(train_index), type(train_index),\"\\n\")\n",
        "                print(\"\\nLength of train_features\",len(train_features), type(train_features),\"\\n\")\n",
        "                print(\"\\n-------------------------*************************************\\n\")\n",
        "\n",
        "                test_features = []\n",
        "                print(\"\\n-------------------------*************************************\\n\")\n",
        "                print(\"\\n Length of  test_features\",len(test_features) , type(test_features),\"\\n\")\n",
        "                print(\"Length of test index\",len(test_index) , type(test_index),\"\\n\")\n",
        "                print(\"\\n-------------------------*************************************\\n\")\n",
        "\n",
        "                for i in train_index:\n",
        "                    print(\"\\n\\n <-----Length of i --->\",i,type(i),\"\\n\")\n",
        "                    train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                    print(\"\\n\\n <-----****************Heyyyyyyyyyyyyyyyy******** --->\",len(train_features),\"\\n\")\n",
        "\n",
        "                    # train_features.append(feature.to_numpy()[images_name.str(onlyfiles[i].split('.')[0])])\n",
        "                    train_label.append(labels_color[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                    \"\"\"#Assuming features is a list where each element represents a feature for an image\n",
        "                    train_features.append(features[i])  # Access feature using index from train_index\n",
        "                    train_label.append(labels_color[i])  # Access label using index from train_index\"\"\"\n",
        "                SVM_Classifier = self.classificatorSVM(train_features, train_label)\n",
        "                DT_Classifier = self.DecisionTree(train_features, train_label)\n",
        "                KNN_Classifier = self.KNN(train_features, train_label)\n",
        "\n",
        "                for i in test_index:\n",
        "                      test_features.append(features[i])  # Access feature using index from test_index\n",
        "                      test_label.append(labels_color[i])\n",
        "                predict_label_SVM = SVM_Classifier.predict(test_features)\n",
        "                predict_label_DT = DT_Classifier.predict(test_features)\n",
        "                predict_label_KNN = KNN_Classifier.predict(test_features)\n",
        "\n",
        "                confusionMatrixSVM = confusion_matrix(test_label, predict_label_SVM)\n",
        "                confusionMatrixDT = confusion_matrix(test_label, predict_label_DT)\n",
        "                confusionMatrixKNN = confusion_matrix(test_label, predict_label_KNN)\n",
        "\n",
        "                classification_report_svm.append(\n",
        "                    classification_report(test_label, predict_label_SVM, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_dt.append(\n",
        "                    classification_report(test_label, predict_label_DT, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_knn.append(\n",
        "                    classification_report(test_label, predict_label_KNN, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "\n",
        "                confusion_matrix_svm.append(confusionMatrixSVM)\n",
        "                confusion_matrix_dt.append(confusionMatrixDT)\n",
        "                confusion_matrix_knn.append(confusionMatrixKNN)\n",
        "\n",
        "                score_accuracy_SVM.append(accuracy_score(test_label, predict_label_SVM))\n",
        "                score_accuracy_DT.append(accuracy_score(test_label, predict_label_DT))\n",
        "                score_accuracy_KNN.append(accuracy_score(test_label, predict_label_KNN))\n",
        "            SVM_RESULTS = [confusion_matrix_svm, classification_report_svm, score_accuracy_SVM]\n",
        "            DT_RESULTS = [confusion_matrix_dt, classification_report_dt, score_accuracy_DT]\n",
        "            KNN_RESULTS = [confusion_matrix_knn, classification_report_knn, score_accuracy_KNN]\n",
        "            SVM.append(SVM_RESULTS)\n",
        "            KNN.append(KNN_RESULTS)\n",
        "            DT.append(DT_RESULTS)\n",
        "        return SVM, KNN, DT\n",
        "\n",
        "    def CrossValidation(self, image, labels, test_size):\n",
        "        X = []\n",
        "        Y = []\n",
        "        X_train, X_test, y_train, y_test = train_test_split(image, labels, test_size=test_size)\n",
        "        X.append(X_train)\n",
        "        X.append(X_test)\n",
        "        Y.append(y_train)\n",
        "        Y.append(y_test)\n",
        "        print()\n",
        "        return X, Y\n",
        "\n",
        "    def ROC_CURVE(self, label_test, label_score):\n",
        "        n_classes = label_test.shape\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(label_test[:, i], label_score[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        # Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_test.ravel(), label_score.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "        # Plot of a ROC curve for a specific class\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver operating characteristic example')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                       ''.format(roc_auc[\"micro\"]))\n",
        "        for i in range(n_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i, roc_auc[i]))\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "R_D_nBsvO9Nf"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WorstCase**\n"
      ],
      "metadata": {
        "id": "9DAskc84OgB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#not good\n",
        "import errno\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "class Classification:\n",
        "    def __init__(self, PATH_PROJECT):\n",
        "        self.path_project = os.path.join(PATH_PROJECT, 'Classification')\n",
        "        try:\n",
        "            if not os.path.exists(self.path_project + 'SVM'):\n",
        "                self.path_getColor = os.path.join(self.path_project, 'SVM')\n",
        "                os.mkdir(self.path_getColor)\n",
        "                print('SVM Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('SVM Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    def readfeatures(self, features_Path):\n",
        "        # featuresFile = open(features_Path, \"r\")\n",
        "        # featuresData = csv.reader(featuresFile)\n",
        "        featuresFile = pd.read_csv(features_Path, sep=',', header=None)\n",
        "        names = featuresFile.iloc[:, 0]\n",
        "        features = featuresFile.iloc[:, 1:]\n",
        "        shapefile = featuresFile.shape\n",
        "        col = []\n",
        "        for x in range(0, shapefile[1]):\n",
        "            if x == 0:\n",
        "                col.append(\"NAME\")\n",
        "            else:\n",
        "                col.append(\"VALOR-\" + str(x))\n",
        "        featuresFile.columns = col\n",
        "        # print(featuresFile)\n",
        "        return names, features\n",
        "\n",
        "    def readLabels(self, labels_path):\n",
        "        labels_path = os.path.join(labels_path, 'Labels.csv')\n",
        "        labels = pd.read_csv(labels_path, sep=',', header=[0])\n",
        "        return labels\n",
        "\n",
        "    def classificatorSVM(self, features, labels):\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        clf = svm.SVC(gamma='scale')\n",
        "        clf.fit(X, tags)\n",
        "        return clf\n",
        "\n",
        "    def DecisionTree(self, features, labels):\n",
        "        dt = DecisionTreeClassifier(random_state=30, max_depth=300)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        dt.fit(X, tags)\n",
        "\n",
        "        return dt\n",
        "\n",
        "    def KNN(self, features, labels):\n",
        "        knn = KNeighborsClassifier(n_neighbors=200, algorithm='auto', weights='distance', n_jobs=-1)\n",
        "        X = []\n",
        "        for f in features:\n",
        "            X.append(f)\n",
        "        tags = []\n",
        "        for tag in labels:\n",
        "            tags.append(tag)\n",
        "        knn.fit(X, tags)\n",
        "        return knn\n",
        "\n",
        "    def classification(self, path_dataset, features, labels, n_splits, tags, target_names, vals_to_replace):\n",
        "        pd.options.mode.chained_assignment = None\n",
        "        onlyfiles = [f for f in listdir(path_dataset) if\n",
        "                     isfile(join(path_dataset, f))]\n",
        "        #print(onlyfiles)\n",
        "        k_folds = KFold(n_splits=n_splits)\n",
        "        SVM = []\n",
        "        KNN = []\n",
        "        DT = []\n",
        "        for stage, feature in zip(labels, features):\n",
        "            stage['Color'] = stage['Color'].map(vals_to_replace)\n",
        "            labels_color = stage['Color'].to_numpy().tolist()\n",
        "            images_name = stage['Nombre de la imagen'].to_numpy().tolist()\n",
        "            svm_training_Score = []\n",
        "            confusion_matrix_svm = []\n",
        "            confusion_matrix_dt = []\n",
        "            confusion_matrix_knn = []\n",
        "            classification_report_svm = []\n",
        "            classification_report_dt = []\n",
        "            classification_report_knn = []\n",
        "            score_accuracy_SVM = []\n",
        "            score_accuracy_DT = []\n",
        "            score_accuracy_KNN = []\n",
        "            index_images_name = []\n",
        "            k_folds.get_n_splits(index_images_name)\n",
        "\n",
        "            for train_index, test_index in k_folds.split(images_name):\n",
        "                train_label = []\n",
        "                test_label = []\n",
        "                train_features = []\n",
        "                test_features = []\n",
        "                for i in train_index:\n",
        "                  try:\n",
        "                      train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                      train_label.append(labels_color[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                  except ValueError:\n",
        "                      print(f\"Warning: '{onlyfiles[i].split('.')[0]}' not found in images_name. Skipping...\")\n",
        "                continue\n",
        "                    #train_features.append(feature.to_numpy().tolist()[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                    # train_features.append(feature.to_numpy()[images_name.str(onlyfiles[i].split('.')[0])])\n",
        "                    #train_label.append(labels_color[images_name.index(onlyfiles[i].split('.')[0])])\n",
        "                SVM_Classifier = self.classificatorSVM(train_features, train_label)\n",
        "                DT_Classifier = self.DecisionTree(train_features, train_label)\n",
        "                KNN_Classifier = self.KNN(train_features, train_label)\n",
        "\n",
        "                for i in test_index:\n",
        "                    test_features.append(feature.to_numpy()[images_name.index(str(onlyfiles[i].split('.')[0]))])\n",
        "                    test_label.append(labels_color[images_name.index(str(onlyfiles[i].split('.')[0]))])\n",
        "                predict_label_SVM = SVM_Classifier.predict(test_features)\n",
        "                predict_label_DT = DT_Classifier.predict(test_features)\n",
        "                predict_label_KNN = KNN_Classifier.predict(test_features)\n",
        "\n",
        "                confusionMatrixSVM = confusion_matrix(test_label, predict_label_SVM)\n",
        "                confusionMatrixDT = confusion_matrix(test_label, predict_label_DT)\n",
        "                confusionMatrixKNN = confusion_matrix(test_label, predict_label_KNN)\n",
        "\n",
        "                classification_report_svm.append(\n",
        "                    classification_report(test_label, predict_label_SVM, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_dt.append(\n",
        "                    classification_report(test_label, predict_label_DT, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "                classification_report_knn.append(\n",
        "                    classification_report(test_label, predict_label_KNN, labels=tags,\n",
        "                                          target_names=target_names, sample_weight=None, digits=5,\n",
        "                                          output_dict=False))\n",
        "\n",
        "                confusion_matrix_svm.append(confusionMatrixSVM)\n",
        "                confusion_matrix_dt.append(confusionMatrixDT)\n",
        "                confusion_matrix_knn.append(confusionMatrixKNN)\n",
        "\n",
        "                score_accuracy_SVM.append(accuracy_score(test_label, predict_label_SVM))\n",
        "                score_accuracy_DT.append(accuracy_score(test_label, predict_label_DT))\n",
        "                score_accuracy_KNN.append(accuracy_score(test_label, predict_label_KNN))\n",
        "            SVM_RESULTS = [confusion_matrix_svm, classification_report_svm, score_accuracy_SVM]\n",
        "            DT_RESULTS = [confusion_matrix_dt, classification_report_dt, score_accuracy_DT]\n",
        "            KNN_RESULTS = [confusion_matrix_knn, classification_report_knn, score_accuracy_KNN]\n",
        "            SVM.append(SVM_RESULTS)\n",
        "            KNN.append(KNN_RESULTS)\n",
        "            DT.append(DT_RESULTS)\n",
        "        return SVM, KNN, DT\n",
        "\n",
        "    def CrossValidation(self, image, labels, test_size):\n",
        "        X = []\n",
        "        Y = []\n",
        "        X_train, X_test, y_train, y_test = train_test_split(image, labels, test_size=test_size)\n",
        "        X.append(X_train)\n",
        "        X.append(X_test)\n",
        "        Y.append(y_train)\n",
        "        Y.append(y_test)\n",
        "        print()\n",
        "        return X, Y\n",
        "\n",
        "    def ROC_CURVE(self, label_test, label_score):\n",
        "        n_classes = label_test.shape\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(label_test[:, i], label_score[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        # Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_test.ravel(), label_score.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "        # Plot of a ROC curve for a specific class\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver operating characteristic example')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.figure()\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                       ''.format(roc_auc[\"micro\"]))\n",
        "        for i in range(n_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i, roc_auc[i]))\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "bTcQSKOyOcud"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#not good\n",
        "\n",
        "import errno\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import Source.Classification as Cl\n",
        "import Source.FeatureExtraction as fE\n",
        "import Source.PreProcessingData as pD\n",
        "import Source.ReadImages as rI\n",
        "\n",
        "\n",
        "def show(image):\n",
        "    cv.imshow('Imagen ', image)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyAllWindows()\n",
        "\n",
        "\n",
        "class MainClass:\n",
        "    PROJECT_PATH = os.path.join(os.getcwd(), os.path.pardir)\n",
        "    PATH_IMAGES_ORIGINAL = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET - Original\"))\n",
        "    PATH_Labels = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"))\n",
        "    PATH_LabelsXML = os.path.abspath(os.path.join(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"Labels\"), \"LabelsXML\"))\n",
        "    PATH_IMAGES = os.path.abspath(\n",
        "        os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), \"DATASET\"))\n",
        "    readimages = None\n",
        "    preprocessing = None\n",
        "    featureExtraction = None\n",
        "    clasification = None\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'PreProcessing'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'PreProcessing'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('PreProcessing Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'FeatureExtraction'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('FeatureExtraction Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Sampling'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Sampling'))\n",
        "                print('Directory Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Sampling Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        try:\n",
        "            if not os.path.exists(self.PROJECT_PATH + 'Classification'):\n",
        "                os.mkdir(os.path.join(self.PROJECT_PATH, 'Classification'))\n",
        "                print('Directory Classification Created')\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                print('Classification Directory Already Exists.')\n",
        "            else:\n",
        "                raise\n",
        "        self.readimages = rI.LoadData(self.PATH_IMAGES_ORIGINAL)\n",
        "        self.preProcessing = pD.PreProcessingData(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.featureExtraction = fE.FeatureExtraction(self.PROJECT_PATH, self.PATH_IMAGES_ORIGINAL)\n",
        "        self.clasification = Cl.Classification(self.PROJECT_PATH)\n",
        "\n",
        "    def main_run(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset ORIGINAL\n",
        "        # img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "        # Se lee el nombre y la imagen que se encuentre en el PATH del dataset RECORTADO\n",
        "        img, name = read.read_One_Image(self.PATH_IMAGES)\n",
        "\n",
        "        # Se obtiene las dimensiones de la imagen original\n",
        "        height_ori, width_ori, depth_ori = img.shape\n",
        "        # print(\"Image original shape: \\n Height:\", height_ori, \", Width:\", width_ori)\n",
        "\n",
        "        # Se realiza un ajuste de tamaño para reducir la imagen a unas dimensiones de 600x400\n",
        "        img_resize = pp.resize_Image(img, name)\n",
        "\n",
        "        # La imagen reajustada se convierte de BGR a RGB\n",
        "        img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "        # Se convierte la imagen de RGB a HSV\n",
        "        hsv_image = pp.rgb_2_HSV(img_resize, name)\n",
        "        # Se saca la imagen en pila de tono de rojos\n",
        "        stack, name = pp.stackColors(hsv_image, name)\n",
        "        # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "        pp.hsv_hist(hsv_image, name)\n",
        "        #\n",
        "        # plt.imshow(img_resize)\n",
        "        # Blur image slightly\n",
        "        name, blurimage = pp.blurImage(img_resize, name)\n",
        "        pp.show_mask(blurimage, name)\n",
        "        pp.overlay_mask(blurimage, img_resize, name)\n",
        "\n",
        "        # Se obtiene la rueda cromatica de la imágen\n",
        "        # pp.getChromatiColor(img_resize,name,fe)\n",
        "\n",
        "        # img_rgb2ycbcr = pp.rgb_2_YCrCb(img_resize, name)\n",
        "        img_rgb2hsv = pp.rgb_2_HSV(img_resize, name)\n",
        "\n",
        "    '''easygui.msgbox(\"Image original shape: \\n Height:\" + str(height_ori) + \"px, Width:\" + str(width_ori) + \"px\" +\n",
        "                   \"\\n Image Resize shape: \\n Height:\" + str(height_res) + \"px, Width:\" + str(width_res) + \"px\",\n",
        "                   image=os.path.join(os.path.join(os.getcwd(), os.path.pardir),\n",
        "                                      'PreProcessing/ResizeImages/' + name),\n",
        "                   title=\"Image Shape - PreProcessing \")'''\n",
        "\n",
        "    def savebin(self):\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        images, names = read.read_Images(self.PATH_IMAGES_P)\n",
        "        for image_point, name_point in zip(images, names):\n",
        "            img_resize = pp.resize_Image(image_point, name_point)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "            img_resize = cv.cvtColor(img_resize, cv.COLOR_RGB2GRAY)\n",
        "            pp.bin(img_resize, name_point)\n",
        "\n",
        "    def main_alldataset(self):\n",
        "        # Se declaran las clases para poder utilizar los elementos\n",
        "        read = self.readimages\n",
        "        pp = self.preProcessing\n",
        "        fe = self.featureExtraction\n",
        "        cc = self.clasification\n",
        "        doption = input(\n",
        "            'Que desea hacer? \\n1. Leer imagenes y obtener caracteristicas'\n",
        "            '\\n2. Leer archivo de caracteristicas y entrenar algoritmo\\n')\n",
        "        option = int(doption)\n",
        "        print(option)\n",
        "\n",
        "        if option == 1:\n",
        "            images, names = read.read_Images(self.PATH_IMAGES)\n",
        "            bar = tqdm(images, ncols=len(images), unit=' image')\n",
        "            for image_point, name_point in zip(bar, names):\n",
        "                bar.set_description(\"Procesando imagen %s\" % name_point)\n",
        "                # Se reajusta la imagen a un tamaño de 600x400px\n",
        "                img_resize = pp.resize_Image(image_point, name_point)\n",
        "\n",
        "                # La imagen reajustada se convierte de BGR a RGB\n",
        "                img_resize = cv.cvtColor(img_resize, cv.COLOR_BGR2RGB)\n",
        "\n",
        "                # Se convierte la imagen de RGB a HSV\n",
        "                hsv_image = pp.rgb_2_HSV(img_resize, name_point)\n",
        "\n",
        "                # Se saca la imagen en pila de tono de rojos\n",
        "                stack, name_point = pp.stackColors(hsv_image, name_point)\n",
        "\n",
        "                # Se saca los histogramas por imagen para determinar el rango de color de los dientes\n",
        "                pp.hsv_hist(hsv_image, name_point)\n",
        "\n",
        "                # Blur image slightly\n",
        "                name_point, blurimage = pp.blurImage(img_resize, name_point)\n",
        "                '''file_ = open(os.path.join(self.PROJECT_PATH, 'Pruebas') + name_point + '.txt', \"w\")\n",
        "                for i in blurimage:\n",
        "                    file_.write(str(i))\n",
        "                file_.close()'''\n",
        "\n",
        "                # A partir del rango de color, se saca una máscara donde se ubican los dientes y se procede a buscar el contorno más grande dentro del área objetivo.\n",
        "                mask = pp.findBiggestContour(blurimage, name_point)\n",
        "                # Se separan los canales de la\n",
        "                # imágen en RGB\n",
        "                channelR, channelG, channelB = cv.split(img_resize)\n",
        "                # Se obtienen los momentos de color de cada espacio de color\n",
        "                red = fe.getFeaturesVector(channelR, mask)\n",
        "                green = fe.getFeaturesVector(channelG, mask)\n",
        "                blue = fe.getFeaturesVector(channelB, mask)\n",
        "                # colores = [\"RED\", \"GREEN\", \"BLUE\"]\n",
        "\n",
        "                # Se obtiene una mascara de la imágen compuesta por los valores de los canales donde se encontraron los datos de interes.\n",
        "                imagen = [red, green, blue]\n",
        "                # Se coloca la ruta del archivo de caracteristicas\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                # Se escribe en un archivo las caracteristicas del dataset\n",
        "                fe.getFeatures(imagen, filefeaturespath, name_point)\n",
        "                fileLabels = open(os.path.join(self.PATH_Labels, 'Labels.csv'))\n",
        "                pp.show_mask(blurimage, name_point)\n",
        "                pp.overlay_mask(blurimage, img_resize, name_point)\n",
        "        elif option == 2:\n",
        "            times_execution = -1\n",
        "            while times_execution <= 0:\n",
        "                times_execution = int(input('\\nIndique la cantidad de veces de ejecución:'))\n",
        "            bar = tqdm(range(times_execution), unit=' times')\n",
        "            for i in bar:\n",
        "                # Se lee el archivo de caracteristicas donde se encuentran los momentos de color\n",
        "                filefeaturespath = os.path.join(os.path.join(self.PROJECT_PATH, 'FeatureExtraction'), 'features.csv')\n",
        "                names, features = cc.readfeatures(filefeaturespath)\n",
        "                labels = cc.readLabels(self.PATH_Labels)\n",
        "                features_images = features.values\n",
        "                vals_to_replace = {'a1': '0', 'a2': '1', 'a3': '2', 'a35': '3'}\n",
        "                tags = ['0', '1', '2', '3']\n",
        "                target_names = ['a1', 'a2', 'a3', 'a35']\n",
        "                folds = int(input('\\nCantidad de folios a seperarar el conjunto de datos:'))\n",
        "                test_size = int(input('\\n Porcentaje de division del conjunto de datos trianing/test:'))\n",
        "                X, Y = cc.CrossValidation(features, labels, test_size / 100)\n",
        "                SVM, DT, KNN = cc.classification(self.PATH_IMAGES, X, Y, folds, tags, target_names, vals_to_replace)\n",
        "                print('--------- Training ---------')\n",
        "                for S, D, K in zip(SVM, DT, KNN):\n",
        "\n",
        "                    matrix_confusion_SVM, report_clasification_SVM, report_scores_SVM = S.split()\n",
        "                    matrix_confusion_DT, report_clasification_DT, report_scores_DT = D.split()\n",
        "                    matrix_confusion_KNN, report_clasification_KNN, report_scores_KNN = K.split()\n",
        "                    print('\\n')\n",
        "                    print('-------SVM------')\n",
        "                    for report in report_clasification_SVM:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN SVM--------')\n",
        "                    print(np.mean(report_scores_SVM))\n",
        "                    print('-------DT------')\n",
        "                    for report in report_clasification_DT:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN DT--------')\n",
        "                    print(np.mean(report_scores_DT))\n",
        "\n",
        "                    print('-------KNN------')\n",
        "                    for report in report_clasification_KNN:\n",
        "                        print(report)\n",
        "                        for item in report:\n",
        "                            print(report[item])\n",
        "                    print('--------MEAN KNN--------')\n",
        "                    print(np.mean(report_scores_KNN))\n",
        "                    print('--------- TEST ---------')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tesis: MainClass = MainClass()\n",
        "    # tesis.main_run()\n",
        "    tesis.main_alldataset()\n",
        "    # tesis.savebin()\n",
        "    print('Se ha finalizado la ejecución del experimento')\n",
        "    sys.exit(0)\n"
      ],
      "metadata": {
        "id": "KfR2DkVbK0tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}